---
title: 'MY474 Summative Assignment #2'
author: '43182'
date: "WT 2025"
output: 
  ioslides_presentation: 
  smaller: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r data_preprocessing, include=FALSE}
# ====== Data Preprocessing ======
library(haven)
library(caret)
library(dplyr)

data <- read_dta("2019_data.dta")

# View data
summary(data) # or str(data)
sum(is.na(data))

# Percent missing per column
missing_percent <- colSums(is.na(data)) / nrow(data) * 100
sort(missing_percent, decreasing = TRUE)

# Remove columns with more than 80% missingness
threshold <- 0.60
data_cleaned <- data[, colMeans(is.na(data)) <= threshold]

# Calculate percent of missingness per row
row_missingness <- rowMeans(is.na(data_cleaned)) * 100
summary(row_missingness)

# Filter rows where missingness is 50% or less
data_cleaned <- data_cleaned[row_missingness <= 50, ]

#Convert coded unanswered i.e. -1 or -2 to NA
data_cleaned[data_cleaned < 0] <- NA

sum(is.na(data_cleaned))

#View column of interest
summary(data_cleaned$h01)
sum(is.na(data_cleaned$h01))

# Remove rows where h01 is missing
data_cleaned <- data_cleaned[!is.na(data_cleaned$h01), ]
# Convert factor levels to numeric (careful: as.numeric() on a factor gives level index, not the label!)
data_cleaned$h01 <- as.numeric(as.character(data_cleaned$h01))

str(data_cleaned$h01)
summary(data_cleaned$h01)

# Convert char columns to factors
# Convert character columns to factors
char_cols <- names(data_cleaned)[sapply(data_cleaned, is.character)]
data_cleaned[char_cols] <- lapply(data_cleaned[char_cols], as.factor)

# Remove predictors with zero variance
nzv <- nearZeroVar(data_cleaned, saveMetrics = TRUE)
data_cleaned <- data_cleaned[, !nzv$zeroVar]

# Tried to apply KNN but seems like cannot find enough neighbours 

# So use median for continuous and mode for categorical
# Function to calculate the mode (most frequent value) of a vector
get_mode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Separate columns by type
numeric_cols <- sapply(data_cleaned, is.numeric)
factor_cols <- sapply(data_cleaned, is.factor)

# For numeric columns, replace NAs with the median
data_cleaned[numeric_cols] <- lapply(data_cleaned[numeric_cols], function(x) {
  if (any(is.na(x))) {
    x[is.na(x)] <- median(x, na.rm = TRUE)
  }
  return(x)
})

# For factor columns, replace NAs with the mode
data_cleaned[factor_cols] <- lapply(data_cleaned[factor_cols], function(x) {
  if (any(is.na(x))) {
    mode_value <- get_mode(x[!is.na(x)])
    x[is.na(x)] <- mode_value
  }
  return(x)
})

# Check if there are any remaining missing values
sum(is.na(data_cleaned))

missing_cells <- is.na(data_cleaned)

# View the rows and columns where the missing values are
which(missing_cells, arr.ind = TRUE)

# Found that it's just interview date so removing that variable

cols_to_remove <- c("a01", "k04", "Constit_Code", "Constit_Name", 
                    "LA_UA_Code", "LA_UA_Name", "interviewer", "Interview_Date", "Stratumlabel", "Stratumlabel2")

data_cleaned <- data_cleaned %>%
  select(-all_of(cols_to_remove))


# Check if there are any remaining missing values
sum(is.na(data_cleaned))

# Check the distribution of the target variable (h01)
table(data_cleaned$h01)
# Convert target to factor
data_cleaned$h01 <- as.factor(data_cleaned$h01)
data_cleaned_filtered <- data_cleaned[data_cleaned$h01 != 0, ]


sum(sapply(data_cleaned, is.numeric))


```

```{r rf_model, include=FALSE}

```

## Survey Response Modelling
  * **Context:** Post-COVID survey fatigue and low response rates
    * Dropping low-value items may improve participation
  * **Key Question:** *Can preferences be inferred from other responses?*
  * **Case Example:** h01 - environment vs economy prioritization from BES 2019

<div style="text-align: center;">
```{r h01_distribution, echo=FALSE, warning=FALSE, fig.height=3, fig.width=7, out.height="30%", out.width="70%"}
library(ggplot2)

# Ensure h01 is a factor with ordered levels (optional but improves display)
data_cleaned$h01 <- factor(data_cleaned$h01, levels = sort(unique(data_cleaned$h01)))

# Create plot object
p <- ggplot(data_cleaned_filtered, aes(x = h01)) +
  geom_bar(fill = "darkslateblue", color = "white", linewidth = 1) +
  labs(
    title = "Class Distribution of H01 (Environment vs Economy)",
    x = "Preference (1 = Environment, 10 = Economy)",
    y = "Number of Respondents"
  ) +
  theme_minimal(base_size = 10) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", fill = NA, size = 0.5)
  )

# Print it explicitly
print(p)
```
</div>


## Modelling Strategy
* Flowchart of prediction approaches
```{r model_flowchart, echo=FALSE, include=TRUE}
library(DiagrammeR)

grViz("
digraph flowchart {
  graph [layout = dot, rankdir = TB, fontsize = 12]

  # Shared root node
  node [shape = box, style = filled, fontcolor = black, color = gray90]
  A [label = 'Prediction model']

  # Multi-class path
  node [shape = box, style = filled, fillcolor = lightyellow, fontcolor = black]
  B1 [label = 'Multi-class classification (11 levels)']
  B2 [label = 'Lasso feature selection (105 features)']
  B3 [label = 'Random Forest with class weights +\nhyperparameter tuning']
  B4 [label = 'Accuracy: 0.27\nPrecision: 0.34\nF1-Score: 0.19']

  # Binary path
  node [shape = box, style = filled, fillcolor = lavender, fontcolor = black]
  C1 [label = 'Binary classification\n(Env: 1–5 vs Econ: 6–10)']
  C2 [label = 'SMOTE to balance classes']
  C3 [label = 'Logistic regression model']
  C4 [label = 'Accuracy: 0.59\nPrecision: 0.44\nF1-Score: 0.50']

  # Edges
  A -> B1
  B1 -> B2
  B2 -> B3
  B3 -> B4

  A -> C1
  C1 -> C2
  C2 -> C3
  C3 -> C4
}
")
```

## Model Performance Evaluation 
  * Loss of nuance in both models
    * Multi-class overpredicts dominant class; binary performs better but still oversimplifies spectrum 
  * h01 likely reflects underlying attitudes not captured in other variables
    * Responses may tap into latent variables
  * Risks of using predicted values
    * Misrepresentation for subgroup analysis and longitudinal studies
    
## Conclusion
  * Prediction cannot replace direct measurement
    * Simplified models overlook nuanced preferences, particularly for minority groups
    
  * Modeling choices: transparency > complexity
    * Prioritized interpretability over higher accuracy
    * Complex models (LLMs, neural networks) excluded due to lack of transparency
    
  * **Recommendation**: Models should account for minority perspectives to ensure accurate representation
    * Consider ensemble methods to improve accuracy for underrepresented groups